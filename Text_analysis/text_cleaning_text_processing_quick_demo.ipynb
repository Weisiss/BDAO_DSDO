{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxsVEJ6FhGgh8wYrNL88Ii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Liping-LZ/BDAO_DSDO/blob/main/Text_analysis/text_cleaning_text_processing_quick_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OVlJcZgwWEe"
      },
      "outputs": [],
      "source": [
        "text = \"BDAO github repository https://github.com/Liping-LZ/BDAO_2223 is ðŸ”¥ðŸ”¥ Coding is SUPER fun!!! Hope I'll do it again. By the way Mark is the best 1234. my email: liping@gmail.com\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lower casing\n",
        "new = text.lower()\n",
        "new"
      ],
      "metadata": {
        "id": "Eb9gXhEUyMqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove contraction\n",
        "!pip install contractions"
      ],
      "metadata": {
        "id": "F5lmOsSGyQM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "new = contractions.fix(new)\n",
        "new"
      ],
      "metadata": {
        "id": "JrqCXvncylCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove emoji & emoticons\n",
        "# First, build a list of commonly used emojis\n",
        "# emoji/emoticons dictionary here: https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
        "import re\n",
        "emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "\n",
        "                      \"]+\", re.UNICODE)\n",
        "\n",
        "new = re.sub(emoj, '', new)\n",
        "new"
      ],
      "metadata": {
        "id": "oSyBCTykztpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove links\n",
        "new = re.sub('http://\\S+|https://\\S+', '', new)\n",
        "new"
      ],
      "metadata": {
        "id": "jfcQKzLe5s0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove email\n",
        "new = re.sub('\\S*@\\S*\\s?', '', new)\n",
        "new"
      ],
      "metadata": {
        "id": "L2ormfui8AtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuation\n",
        "import string\n",
        "import re\n",
        "new = re.sub('[%s]' % re.escape(string.punctuation), ' ', new)\n",
        "new"
      ],
      "metadata": {
        "id": "oQnpPYWCyuK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove non-word characters, so numbers and ___ etc\n",
        "new = re.sub(\"[^A-Za-z]\", \" \", new)\n",
        "new"
      ],
      "metadata": {
        "id": "zJZHVcZJ9Oyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove extra space\n",
        "new = re.sub('\\s{2,}', ' ', new)\n",
        "new"
      ],
      "metadata": {
        "id": "CVXQhT2T1-gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "cRjmXSZk3L7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenise the words\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_token = word_tokenize(new)\n",
        "\n",
        "word_token"
      ],
      "metadata": {
        "id": "lRd5whf9_zo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start dealing with stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "5GbopT1pBepX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "# use English stopwords\n",
        "stopwords = stopwords.words(\"english\")"
      ],
      "metadata": {
        "id": "7iQh6hesBi03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_without_stopwords = [w for w in word_token if not w in stopwords]\n",
        "token_without_stopwords"
      ],
      "metadata": {
        "id": "ph4qAznMB14w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# option 1: stem the tokens\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "text = ['managers', 'managing', 'management', 'manage','manages','managed']\n",
        "stemmed = [ps.stem(w) for w in text]\n",
        "stemmed"
      ],
      "metadata": {
        "id": "NDNUgyW7CuOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# option 2: lemmatise the tokens\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "n_lemmatised = [lemmatizer.lemmatize(w) for w in text]\n",
        "v_lemmatised = [lemmatizer.lemmatize(w,'v') for w in n_lemmatised]\n",
        "r_lemmatised = [lemmatizer.lemmatize(w,'r') for w in v_lemmatised]\n",
        "a_lemmatised = [lemmatizer.lemmatize(w,'a') for w in r_lemmatised]\n",
        "\n",
        "a_lemmatised"
      ],
      "metadata": {
        "id": "ER3iVizvDG5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_token = [ps.stem(w) for w in token_without_stopwords]\n",
        "stemmed_token"
      ],
      "metadata": {
        "id": "zf2fUK5QImMr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}