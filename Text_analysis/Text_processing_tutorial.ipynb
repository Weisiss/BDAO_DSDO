{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3H4O0UBepSRU"
      ],
      "authorship_tag": "ABX9TyOis3AamB6WCwAErRXhjt2h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Liping-LZ/BDAO_DSDO/blob/main/Text_analysis/Text_processing_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. What is Natural Language Processing (NLP)**\n",
        "\n",
        "Natural Language Processing (NLP) is broadly defined as the automatic manipulation of natural language, like speech and text, by software. In other words, NLP is an important technique to help understand human language. NLP is a broad topic but we are mainly talking about how to use NLP techniques to do text mining and text analysis. In this tutorial, we will talk about text cleaning and data text processing, which are the essential steps to get data prepared for further text mining.\n",
        "\n"
      ],
      "metadata": {
        "id": "_YIfml6R0NK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Text processing**\n",
        "Usually, we need to do some proper text cleanning and processing before text mining (e.g. topic modeling). Here are the common steps to follow:\n",
        "\n",
        "#### **Step 1: Cleaning text**\n",
        "- (optional) Encoding. Usually you don't need to set up the encoding type or just set it as \"utf-8\" (which is suitable for all languages), but sometimes if your text data is non-English, you might need to look for the right encoding type. For example, \"latin_1\" is suitable for the languages in Western Europe and \"gbk\" for Chinese. Here is the [List of Python standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).\n",
        "- Lower casing;\n",
        "- Remove special characters, such as emoji;\n",
        "- Remove email address and url;\n",
        "- Remove punctuation\n",
        "\n",
        "#### **Step 2: Tokenisation**\n",
        "In this step, the text is split into smaller units. Sentence-->words.\n",
        "\n",
        "#### **Step 3: Remove stop words**\n",
        "stop words are a set of commonly used words in a language. Example of stop words in English are \"is\", \"a\", \"the\" and etc. These words are usually not useful, so we normally remove them.\n",
        "\n",
        "#### **Step 4: Stemming or Lemmatisation**\n",
        "Stemming is the text standardization step where the words are stemmed or diminished to their root/base form. For example, words like ‘programmer’, ‘programming, ‘program’ will be stemmed to ‘program’. But the disadvantage of stemming is that it stems the words such that its root form loses the meaning or it is not diminished to a proper English word. For example, \"manages\" will be stemmed to \"manag\".\n",
        "\n",
        "Lemmatisation also stems the words but try to make sure the words are not losing their meaning.Lemmatization has a pre-defined dictionary that stores the context of words and checks the word in the dictionary while diminishing. Thus, the words make more sense in this case, but lemmatisation might take longer to run.\n",
        "\n",
        "We don't need to use both, but which one to choose? It depends. Sometimes stemming works fine then it's more effective. But if we need to get the actual meaning with actual words, then lemmatisation is more suitable.\n",
        "\n",
        "#### **Step 5: Once the processing are done, put the tokens back together as text**"
      ],
      "metadata": {
        "id": "hIV1vdx71GPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Practice with data example**"
      ],
      "metadata": {
        "id": "NTFRFSSgJXRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's import data first\n",
        "# Run the code and upload the csv file from your laptop\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "QKVnAv_LKyCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# once you upload the data to the cloud, you can read the data into dataframe.\n",
        "# Remember, if you close the notebook or disconnect, you need to upload the file (run the code above) again before you read data.\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "data = 'your_file_name' # change the csv file name to your file name that you uploaded\n",
        "df = pd.read_csv(data)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "BwX1YiZ3LygV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change your target_column to the column with review data\n",
        "\n",
        "target_column = 'your_target_column'"
      ],
      "metadata": {
        "id": "eCz0-gAlb0t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1 Text cleaning**"
      ],
      "metadata": {
        "id": "a340R2knJi9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop data with missing values in the 'content' column (target_column)\n",
        "df = df.dropna(subset=[target_column])"
      ],
      "metadata": {
        "id": "fmGndStlKHPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the relevant column to lowercase\n",
        "df[target_column] = df[target_column].str.lower()"
      ],
      "metadata": {
        "id": "sY06_KI1MQIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove contraction\n",
        "!pip install contractions\n",
        "import contractions\n",
        "df[target_column] = df[target_column].map(lambda x: contractions.fix(x))"
      ],
      "metadata": {
        "id": "719romQM9-4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unwanted characters/objects (e.g. url, email, emoji etc.). You don't need to run all of them, but just take the ones suitable for your case.\n",
        "# import the re library. Re is a built-in python package, which can be used to work with Regular Expressions.\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "# Remove new line characters (\"/n\")\n",
        "\n",
        "df[target_column] = df[target_column].map(lambda x: re.sub('\\s+', ' ', x))\n",
        "\n",
        "# Remove url link\n",
        "df[target_column] = df[target_column].apply(lambda x: re.sub('http://\\S+|https://\\S+', '', x))\n",
        "\n",
        "# Remove email address\n",
        "df[target_column] = df[target_column].map(lambda x: re.sub('\\S*@\\S*\\s?', '', x))\n",
        "\n",
        "# Remove emoji\n",
        "# First, build a list of commonly used emojis\n",
        "emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "\n",
        "# Then remove the emoji\n",
        "df[target_column] = df[target_column].map(lambda x: re.sub(emoj, ' ', x))\n",
        "\n",
        "# Remove non-word characters, so numbers and ___ etc\n",
        "df[target_column] = df[target_column].str.replace(\"[^A-Za-z]\", \" \", regex = True)\n",
        "\n",
        "# Remove overspace\n",
        "df[target_column] = df[target_column].map(lambda x: re.sub('\\s{2,}', \" \", x))"
      ],
      "metadata": {
        "id": "pI0eXgP8M_sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation\n",
        "import string\n",
        "df[target_column] = df[target_column].map(lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))"
      ],
      "metadata": {
        "id": "FFVu-bk9R6CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2 Tokenisation & stop-words removal**"
      ],
      "metadata": {
        "id": "tXJ5FjkMS4ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "o8YJEwfrTnzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of the reviews from the 'Review' column\n",
        "words = df[target_column].tolist()\n",
        "\n",
        "# tokenise the words\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "word_tokens = [] # create an empty list to put the tokenised words\n",
        "for review in words:\n",
        "    word_tokens.append(word_tokenize(review))\n",
        "\n",
        "word_tokens[0]"
      ],
      "metadata": {
        "id": "L8haEbDENewO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start dealing with stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "3dIF1zDpUFSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "# use English stopwords\n",
        "stopwords = stopwords.words(\"english\")\n",
        "# you can also extend the stopwords list with more words that you want to remove\n",
        "stopwords.extend([''])\n",
        "\n",
        "# create a new list with stop words removed\n",
        "tokens_without_stopwords = []\n",
        "for review in word_tokens:\n",
        "    tokens_without_stopwords.append([w for w in review if not w in stopwords])\n",
        "\n",
        "tokens_without_stopwords[0]"
      ],
      "metadata": {
        "id": "A_pKfuEiTsvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3 Stemming or lemmatisation**"
      ],
      "metadata": {
        "id": "mWJpMPlxVXVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# option 1: stem the tokens\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "stemmed = []\n",
        "for review in tokens_without_stopwords:\n",
        "    stemmed.append([ps.stem(w) for w in review])\n",
        "\n",
        "stemmed[0]"
      ],
      "metadata": {
        "id": "3odZkDXvVWlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# option 2: lemmatise the tokens\n",
        "\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "HsZAJ633Vrkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# The lemmatizer requires the correct POS tag to be accurate, if you use the default settings of the WordNetLemmatizer.lemmatize(), the default tag is noun.\n",
        "# Which means the words other than noun would not be lemmatised, and this is not what we expect.\n",
        "# Therefore we need to define the POS tags ('n' for nouns(default), 'a' for adj, 'v' for verb, 'r' for adverb) which are allowed to be lemmatised.\n",
        "\n",
        "lemmatised = []\n",
        "for review in tokens_without_stopwords:\n",
        "  n_lemmatised = [lemmatizer.lemmatize(w) for w in review]\n",
        "  v_lemmatised = [lemmatizer.lemmatize(w,'v') for w in n_lemmatised]\n",
        "  r_lemmatised = [lemmatizer.lemmatize(w,'r') for w in v_lemmatised]\n",
        "  a_lemmatised = [lemmatizer.lemmatize(w,'a') for w in r_lemmatised]\n",
        "  lemmatised.append(a_lemmatised) # here noun, adj, verb, adv are the POS tags that we allowed\n",
        "\n",
        "lemmatised[0]"
      ],
      "metadata": {
        "id": "hFjYBNKKWqY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# put the tokens back together as text after stemming\n",
        "import string\n",
        "rejoin = []\n",
        "for review in stemmed:\n",
        "    x = \" \".join(review) # join the text back together\n",
        "    # remove punctuation from the reviews using the string package\n",
        "    rejoin.append(x)\n",
        "\n",
        "# add the reformed text to the data frame\n",
        "df['filtered_review'] = rejoin"
      ],
      "metadata": {
        "id": "5MVv9JZyksXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# put the tokens back together as text after lemmatisation\n",
        "import string\n",
        "rejoin_2 = []\n",
        "for review in lemmatised: # Here we choose to use stemming instead of lemmatisation\n",
        "    x = \" \".join(review) # join the text back together\n",
        "    # remove punctuation from the reviews using the string package\n",
        "    rejoin_2.append(x)\n",
        "\n",
        "# add the reformed text to the data frame\n",
        "df['filtered_review_2'] = rejoin_2"
      ],
      "metadata": {
        "id": "jyqfc11yntTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head() # new columns are added to the original data"
      ],
      "metadata": {
        "id": "Zow9UA-iniYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Basic text analysis**"
      ],
      "metadata": {
        "id": "UdDq8ii8ot51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1 POS Tagging**\n",
        "Part-of-Speech Tagging (POS tagging) is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech based on both its definition and its context. To put it simple, POS tagging help us mark up a word as nouns, verbs, adjectives or adverbs. We can also extract specific type of words based on what we need."
      ],
      "metadata": {
        "id": "3H4O0UBepSRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before running the module, we need to download some packages and list of tags available in nltk\n",
        "nltk.download('tagsets')\n",
        "print(nltk.help.upenn_tagset())\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "Xm_PESeuo7a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag, RegexpParser\n",
        "\n",
        "# Here we define the tags for each word in the token list\n",
        "tags = []\n",
        "for i in range(len(word_tokens)): # here we use the tokenised words from the review without removing stopwords or stemming or lemmatisation because POS tagging needs to consider the context\n",
        "    tags.append(pos_tag(word_tokens[i]))\n",
        "\n",
        "tags[0]\n"
      ],
      "metadata": {
        "id": "SGseGO6xqmEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we extract the nouns from the tokens, added to list\n",
        "noun = []\n",
        "for i in range(len(word_tokens)): # here we use the tokenised words from the review without removing stopwords or stemming or lemmatisation because POS tagging needs to consider the context\n",
        "    noun.append([word for word,pos in pos_tag(word_tokens[i]) if pos.startswith('N')])\n",
        "\n",
        "noun[0]"
      ],
      "metadata": {
        "id": "dLrfzPGJtOMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we extract the adjective from the tokens, added to list\n",
        "adj = []\n",
        "for i in range(len(word_tokens)): # here we use the tokenised words from the review without removing stopwords or stemming or lemmatisation because POS tagging needs to consider the context\n",
        "    adj.append([word for word,pos in pos_tag(word_tokens[i]) if pos == 'JJ'])\n",
        "\n",
        "adj[0]"
      ],
      "metadata": {
        "id": "Bq6A-SxkuJRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Notes:**\n",
        "By POS Tagging, we can understand what types of words are in each review. And this allows us to extract only noun or only adjective, so we can better categorize the words and do further analysis. For example, we can extract specific type of words to check the frequency of words (e.g. nouns usually give more information about what). This is not a required step in the text processing, but it can be a tool for text analysis.\n",
        "\n",
        "(POS tagging by nltk is not 100% accurate, so it's common if you see some words are not allocated with right tags.)"
      ],
      "metadata": {
        "id": "RR9i2k8xxL3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.2 Text vectorisation**\n",
        "\n",
        "Text Vectorisation is the process of converting text into numerical representation. Once we convert text into numbers, then they can be usable features to be put into models. And with numbers we can do more further analysis and interpretation of the data as well.\n",
        "\n",
        "Here we are introducing two methods of verctorisation:\n",
        "- Term Document Matrix: create a binary vector where each index denotes the presence or absence of a word;\n",
        "- Term Frequency-Inverse Document Frequency: weight each word by some form of importance score as well when creating term document matrix."
      ],
      "metadata": {
        "id": "ImJi1TyPy5kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For creating Term Document Matrix (DTM), CountVectorizer() from scikit-learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# set vectorizer - CountVectorizer for word counts\n",
        "vectorizer = CountVectorizer(stop_words = ([\"app\"]))\n",
        "\n",
        "# create an array of word counts\n",
        "doc_vec = vectorizer.fit_transform(df.filtered_review_2)\n",
        "\n",
        "# convert this to a dataframe\n",
        "df2 = pd.DataFrame(doc_vec.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# set a threshold to drop infrequent words\n",
        "threshold = 0.1\n",
        "\n",
        "# drop words based on the threshold\n",
        "df2 = df2.drop(df2.mean()[df2.mean() < threshold].index.values, axis=1) # Here find out the word with average word count lower than 0.1 and drop them\n",
        "\n",
        "# join the two datasets together\n",
        "dtm = df.join(df2, how='left',lsuffix='_left', rsuffix='_right')"
      ],
      "metadata": {
        "id": "mkor3CUOy4sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.mean()"
      ],
      "metadata": {
        "id": "q5piuq6Arux3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the dataframe out to csv and download\n",
        "dtm.to_csv('dtm.csv',index=False)\n",
        "files.download('dtm.csv')"
      ],
      "metadata": {
        "id": "hCnhsG2ppFs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use the same process to creat document term matrix by considering the importance of the words\n",
        "# Here we will use TfidfVectorizer() from scikit-learn to convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# set vectorizer - CountVectorizer for word counts\n",
        "tfidfvectorizer = TfidfVectorizer(stop_words = ([\"app\"]))\n",
        "\n",
        "# create an array of word counts\n",
        "vec = tfidfvectorizer.fit_transform(df.filtered_review_2)\n",
        "\n",
        "# convert this to a dataframe\n",
        "df3 = pd.DataFrame(vec.toarray(), columns=tfidfvectorizer.get_feature_names_out())\n",
        "\n",
        "# set a threshold to drop infrequent words\n",
        "threshold = 0.01\n",
        "\n",
        "# drop words based on the threshold\n",
        "df3 = df3.drop(df3.mean()[df3.mean() < threshold].index.values, axis=1) # Here find out the word with average word score lower than 0.001 and drop them\n",
        "\n",
        "# join the two datasets together\n",
        "tfidf = df.join(df3, how='left',lsuffix='_left', rsuffix='_right')"
      ],
      "metadata": {
        "id": "Kw3m65zv140T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.mean()"
      ],
      "metadata": {
        "id": "esJx4ruzqV_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf.head()"
      ],
      "metadata": {
        "id": "6t1_CE2mp68s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the dataframe out to csv and download\n",
        "tfidf.to_csv('tfidf.csv',index=False)\n",
        "files.download('tfidf.csv')"
      ],
      "metadata": {
        "id": "x3CyaZDApfHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.3 Visualisation based on the matrix**"
      ],
      "metadata": {
        "id": "PMim9gzb5iuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words = ([\"app\"]))\n",
        "doc_vec = vectorizer.fit_transform(df.filtered_review_2)\n",
        "count_df = pd.DataFrame(doc_vec.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "Cloud = WordCloud(background_color=\"white\", max_words=100).generate_from_frequencies(count_df.T.sum(axis=1))\n",
        "plt.figure(figsize=(10,5),facecolor ='k')\n",
        "plt.imshow(Cloud, interpolation ='bilinear')\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zFWGHr2axZwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words = ([\"app\"]))\n",
        "doc_vec = vectorizer.fit_transform(df.filtered_review_2)\n",
        "tfidf_df = pd.DataFrame(doc_vec.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "Cloud = WordCloud(background_color=\"white\", max_words=100).generate_from_frequencies(tfidf_df.T.sum(axis=1))\n",
        "plt.figure(figsize=(10,5),facecolor ='k')\n",
        "plt.imshow(Cloud, interpolation ='bilinear')\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ByOxui4GvtZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n_words(corpus, n=None):\n",
        "    \"\"\"\n",
        "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
        "\n",
        "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) ->\n",
        "    [('python', 2),\n",
        "     ('world', 2),\n",
        "     ('love', 2),\n",
        "     ('hello', 1),\n",
        "     ('is', 1),\n",
        "     ('programming', 1),\n",
        "     ('the', 1),\n",
        "     ('language', 1)]\n",
        "    \"\"\"\n",
        "    vec = CountVectorizer(stop_words = ([\"app\"])).fit(corpus) # you can also change it to tfidfvectorizer, then you are looking for top words based on tf-idf\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "top_20_words = get_top_n_words(df['filtered_review_2'], 20)\n",
        "for word, freq in top_20_words:\n",
        "    print(word, freq)"
      ],
      "metadata": {
        "id": "8H_GeUqJ6Bi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "top20_df = pd.DataFrame(top_20_words, columns=['word', 'count'])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 8))\n",
        "# Plot horizontal bar graph\n",
        "top20_df.plot.bar(x='word',y='count',ax=ax,color=\"grey\")\n",
        "\n",
        "ax.set_title(\"Top 20 Words Found in Reviews\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XtB1gY8_7qxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n_words(corpus, n=None): # Based on TF-IDF score\n",
        "    \"\"\"\n",
        "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
        "\n",
        "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) ->\n",
        "    [('python', 2),\n",
        "     ('world', 2),\n",
        "     ('love', 2),\n",
        "     ('hello', 1),\n",
        "     ('is', 1),\n",
        "     ('programming', 1),\n",
        "     ('the', 1),\n",
        "     ('language', 1)]\n",
        "    \"\"\"\n",
        "    vec = TfidfVectorizer(stop_words = ([\"app\"])).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "top_20_words = get_top_n_words(df['filtered_review_2'], 20)\n",
        "for word, freq in top_20_words:\n",
        "    print(word, freq)"
      ],
      "metadata": {
        "id": "UKH7qKp-uIBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "top20_df = pd.DataFrame(top_20_words, columns=['word', 'count'])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 8))\n",
        "# Plot horizontal bar graph\n",
        "top20_df.plot.bar(x='word',y='count',ax=ax,color=\"grey\")\n",
        "\n",
        "ax.set_title(\"Top 20 Words Found in Reviews\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EN8gnUOLumTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "long_string = ' '.join(list(df['filtered_review']))\n",
        "wordcloud = WordCloud(width = 480, height = 540, background_color= 'white', max_words=500, contour_width=3, contour_color= 'steelblue').generate(long_string)\n",
        "plt.figure(figsize=(20,10),facecolor ='k')\n",
        "plt.imshow(wordcloud, interpolation ='bilinear')\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0d1YbUgF83j5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}