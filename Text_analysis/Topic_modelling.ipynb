{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI5JuHxuJ0YQbEpNxRoCGC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Liping-LZ/BDAO_DSDO/blob/main/Text_analysis/Topic_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic Modelling for review analysis**\n",
        "\n",
        "Latent Dirichlet Allocation (LDA) is a classic model to do topic modelling. Topic modeling is unsupervised learning and the goal is to group different documents to the same “topic”."
      ],
      "metadata": {
        "id": "7vmwZEW9_PUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. import data you would like to analyse**"
      ],
      "metadata": {
        "id": "CCwhAcbc_7NO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLAdb7rY-qUG"
      },
      "outputs": [],
      "source": [
        "# Let's import data first\n",
        "# Run the code and upload the csv file from your laptop\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = 'your_file_name'\n",
        "df = pd.read_csv(data) # change the csv file name to your file name that you uploaded\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YDl5IA5VMHtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_column = 'your_target_column' #change the column name to where the review is"
      ],
      "metadata": {
        "id": "MIVF0TNWAu9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Text cleaning & Text processing**\n",
        "Before you do topic modeling, you need make sure you clean and process the text."
      ],
      "metadata": {
        "id": "FwEqMOqvCeKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "import re\n",
        "import string\n",
        "import contractions\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import spacy\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "# drop data with missing values in the 'content' column\n",
        "df = df.dropna(subset=[target_column])\n",
        "\n",
        "# drop duplicate review content\n",
        "df = df.drop_duplicates(subset=[target_column])\n",
        "\n",
        "# remove contraction\n",
        "df[target_column] = df[target_column].map(lambda x: contractions.fix(x))\n",
        "\n",
        "# convert the relevant column to lowercase\n",
        "df[target_column] = df[target_column].str.lower()\n",
        "\n",
        "# Remove overspace\n",
        "df[target_column] = df[target_column].map(lambda x: re.sub('\\s{2,}', \" \", x))\n",
        "\n",
        "# Remove non-word characters, so numbers and ___ etc\n",
        "df[target_column] = df[target_column].str.replace(\"[^A-Za-z]\", \" \", regex = True)\n",
        "\n",
        "# Remove punctuation\n",
        "df[target_column] = df[target_column].map(lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))"
      ],
      "metadata": {
        "id": "eEnsrDs8A2eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of the contents from the 'contents' column\n",
        "words = df[target_column].tolist()\n",
        "\n",
        "# tokenise the words\n",
        "word_tokens = []\n",
        "for content in words:\n",
        "    word_tokens.append(word_tokenize(content))\n",
        "\n",
        "# create bigram model\n",
        "bigram = gensim.models.phrases.Phrases(word_tokens, min_count=3, threshold=10)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram) # Faster way to get a sentence clubbed as a trigram/bigram\n",
        "\n",
        "# NLTK Stop words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['flight']) #add more stopwords here\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "# Define functions for stopwords, bigrams and lemmatisation\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n",
        "\n",
        "# Remove stopwords\n",
        "data_words_nostops = remove_stopwords(word_tokens)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Do lemmatisation keeping only noun, adj, vb, adv\n",
        "data_lemmatised = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "# put the tokens back together as text to have our filtered contents\n",
        "\n",
        "rejoin = []\n",
        "for content in data_lemmatised: # Here we choose to use stemming instead of lemmatisation\n",
        "    x = \" \".join(content) # join the text back together\n",
        "    rejoin.append(x)\n",
        "\n",
        "# add the reformed text to the data frame\n",
        "df['cleaned_review'] = rejoin"
      ],
      "metadata": {
        "id": "phOzbFcrMdvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "KhEtZvddebij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Build the LDA model**"
      ],
      "metadata": {
        "id": "g_MX7WkJJN-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorise the data into word counts\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "\n",
        "max_words = 1000 #how many words taking account for topic modeling\n",
        "vectorizer = CountVectorizer(max_features=max_words)\n",
        "vec = vectorizer.fit_transform(df['cleaned_review'])\n",
        "\n",
        "k = 6 #this is the number of the topic. you can decide the number\n",
        "\n",
        "lda = LDA(n_components=k, max_iter=5, learning_method='online', random_state = 10)\n",
        "lda.fit(vec)"
      ],
      "metadata": {
        "id": "L0SAqJIzDuba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Visualisation of the topics**"
      ],
      "metadata": {
        "id": "-n28wuX2JXq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import wordcloud\n",
        "\n",
        "#declaring number of terms we need per topic\n",
        "terms_count = 50\n",
        "\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "wcloud = wordcloud.WordCloud(background_color=\"White\",mask=None, max_words=100,\\\n",
        "                             max_font_size=60,min_font_size=10,prefer_horizontal=0.9,\n",
        "                             contour_width=3,contour_color='Black',colormap='Set2')\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(30, 15), sharex=True)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx,topic in enumerate(lda.components_):\n",
        "    print('Topic# ',idx+1)\n",
        "    abs_topic = abs(topic)\n",
        "    topic_terms = [[terms[i],topic[i]] for i in abs_topic.argsort()[:-terms_count-1:-1]]\n",
        "    topic_terms_sorted = [[terms[i], topic[i]] for i in abs_topic.argsort()[:-terms_count - 1:-1]]\n",
        "    topic_words = []\n",
        "    for i in range(terms_count):\n",
        "        topic_words.append(topic_terms_sorted[i][0])\n",
        "    print(','.join( word for word in topic_words))\n",
        "    print(\"\")\n",
        "    dict_word_frequency = {}\n",
        "\n",
        "    for i in range(terms_count):\n",
        "        dict_word_frequency[topic_terms_sorted[i][0]] = topic_terms_sorted[i][1]\n",
        "\n",
        "    ax = axes[idx]\n",
        "    ax.set_title(f'Topic {idx +1}',fontdict={'fontsize': 30})\n",
        "    wcloud.generate_from_frequencies(dict_word_frequency)\n",
        "    ax.imshow(wcloud, interpolation='bilinear')\n",
        "    ax.axis(\"off\")"
      ],
      "metadata": {
        "id": "elXstxIYGzct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise the result into bar charts in topic\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# helper function to plot topics\n",
        "# see Grisel et al.\n",
        "# https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
        "def plot_top_words(model, feature_names, n_top_words, title):\n",
        "    fig, axes = plt.subplots(1, 6, figsize=(30, 15), sharex=True)\n",
        "    axes = axes.flatten()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
        "        top_features = [feature_names[i] for i in top_features_ind]\n",
        "        weights = topic[top_features_ind]\n",
        "\n",
        "        ax = axes[topic_idx]\n",
        "        ax.barh(top_features, weights, height=0.7)\n",
        "        ax.set_title(f'Topic {topic_idx +1}',\n",
        "                     fontdict={'fontsize': 30})\n",
        "        ax.invert_yaxis()\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "        for i in 'top right left'.split():\n",
        "            ax.spines[i].set_visible(False)\n",
        "        fig.suptitle(title, fontsize=40)\n",
        "\n",
        "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
        "    plt.show()\n",
        "\n",
        "n_top_words = 20  #how many words to be visualised in each topic\n",
        "\n",
        "# get the list of words (feature names)\n",
        "vec_feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# print the top words per topic\n",
        "plot_top_words(lda, vec_feature_names, n_top_words, 'Topics in LDA model')"
      ],
      "metadata": {
        "id": "dKV08bKSEoYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Download file with assigned topic**"
      ],
      "metadata": {
        "id": "UpRn6KhZg1xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_topic = lda.transform(vec)\n",
        "docsVStopics = pd.DataFrame(doc_topic, columns=[\"Topic\"+str(i+1) for i in range(k)])\n",
        "df = df.join(docsVStopics)\n",
        "df['mostlikely_topic'] = docsVStopics.idxmax(axis=1)\n",
        "\n",
        "df.to_csv('topic_modeling_result.csv', index=False) # save the file to google drive\n",
        "files.download('topic_modeling_result.csv') # download the file to your local machine"
      ],
      "metadata": {
        "id": "SXEQdUAVgtmY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}